{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스 기사 분류: 다중 분류 문제\n",
    "\n",
    "## 추가 실험\n",
    "\n",
    "* 더 크거나 작은 층을 사용해 보세요: 32개 유닛, 128개 유닛 등 (교재에서는 64, 64, 46 썼음. 출력층은 46으로 고정)\n",
    "* 교재의 예시에서는 두 개의 은닉층을 사용했습니다. 한 개의 은닉층이나 세 개의 은닉층을 사용해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leehyunjoo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def vectorize_sequences(sequences, dimenstion=10000):\n",
    "    results = np.zeros((len(sequences), dimenstion))\n",
    "    for i, sequences in enumerate(sequences):\n",
    "        results[i,sequences] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영화 이진 분류에서는 레이블이 0, 1 로 2개였는데, 이번 뉴스 기사 분류는 레이블이 46개이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블을 벡터로 변환하는 방법1)\n",
    "# 방법1 말고 방법2 실행할 것임 (어짜피 해도 똑같긴함)\n",
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블을 벡터로 변환하는 방법2) (그냥 똑같으니까 실행해도 상관없음)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 118us/step - loss: 2.6213 - accuracy: 0.5481 - val_loss: 1.7195 - val_accuracy: 0.6560\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 101us/step - loss: 1.3986 - accuracy: 0.7161 - val_loss: 1.3020 - val_accuracy: 0.7220\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 97us/step - loss: 1.0257 - accuracy: 0.7845 - val_loss: 1.1370 - val_accuracy: 0.7550\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 97us/step - loss: 0.8087 - accuracy: 0.8297 - val_loss: 1.0534 - val_accuracy: 0.7640\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 96us/step - loss: 0.6483 - accuracy: 0.8642 - val_loss: 0.9864 - val_accuracy: 0.7900\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 104us/step - loss: 0.5182 - accuracy: 0.8936 - val_loss: 0.9463 - val_accuracy: 0.8000\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 98us/step - loss: 0.4183 - accuracy: 0.9153 - val_loss: 0.9361 - val_accuracy: 0.8040\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 101us/step - loss: 0.3424 - accuracy: 0.9280 - val_loss: 0.9167 - val_accuracy: 0.8060\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 105us/step - loss: 0.2837 - accuracy: 0.9382 - val_loss: 0.9145 - val_accuracy: 0.8160\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 101us/step - loss: 0.2389 - accuracy: 0.9454 - val_loss: 0.9128 - val_accuracy: 0.8230\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 99us/step - loss: 0.2061 - accuracy: 0.9479 - val_loss: 0.9762 - val_accuracy: 0.8070\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 99us/step - loss: 0.1813 - accuracy: 0.9525 - val_loss: 0.9581 - val_accuracy: 0.8060\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 99us/step - loss: 0.1646 - accuracy: 0.9518 - val_loss: 1.0634 - val_accuracy: 0.7830\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 99us/step - loss: 0.1477 - accuracy: 0.9543 - val_loss: 1.0092 - val_accuracy: 0.8060\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 101us/step - loss: 0.1426 - accuracy: 0.9570 - val_loss: 1.0254 - val_accuracy: 0.8010\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 99us/step - loss: 0.1289 - accuracy: 0.9578 - val_loss: 1.0563 - val_accuracy: 0.7980\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 100us/step - loss: 0.1252 - accuracy: 0.9557 - val_loss: 1.0248 - val_accuracy: 0.8120\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 101us/step - loss: 0.1202 - accuracy: 0.9568 - val_loss: 1.0842 - val_accuracy: 0.8030\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 97us/step - loss: 0.1158 - accuracy: 0.9568 - val_loss: 1.1081 - val_accuracy: 0.7930\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 98us/step - loss: 0.1139 - accuracy: 0.9563 - val_loss: 1.0741 - val_accuracy: 0.8020\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# 기본 예제\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 컴파일을 한 후에 훈련을 검증해보는 것이다.\n",
    "# 훈련 데이터 (train)에서 1000개의 샘플을 떼어내서 검증(validation) 세트로 사용한다.\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "\n",
    "history = model.fit(partial_x_train,partial_y_train,\n",
    "                    epochs=20, batch_size=512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16번하면 역전되니까 16로 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가 문제 : 더 작은 층을 사용해보자, 1개의 은닉층만 사용해보자.\n",
    "# 마지막 Dense 층이 46인 것은, 각 입력 샘플에 대해 46차원의 벡터를 출력한다는 뜻\n",
    "# softmax 함수는, 46일때, 46차원의 출력벡터를 만들고,\n",
    "# output[i] 는 어떤 샘플이 클래스 i에 속할 확률임. 46개의 값을 다 더하면 1 이다.\n",
    "\n",
    "# 영화 이진 분류에서는 16차원을 사용했었는데, 그때는 출력 클래스의 종류가 2개였다.\n",
    "# 그런데 여기서는 클래스가 46개이기 때문에, 16차원으로는 충분하지 않을 것 같아. 그래서 64 차원을 사용했다.\n",
    "# Q. 차원은 16의 배수로 하는 것인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 1000 samples\n",
      "Epoch 1/16\n",
      "8982/8982 [==============================] - 1s 113us/step - loss: 2.5583 - accuracy: 0.5638 - val_loss: 1.5117 - val_accuracy: 0.6930\n",
      "Epoch 2/16\n",
      "8982/8982 [==============================] - 1s 99us/step - loss: 1.3403 - accuracy: 0.7168 - val_loss: 1.0404 - val_accuracy: 0.7640\n",
      "Epoch 3/16\n",
      "8982/8982 [==============================] - 1s 97us/step - loss: 0.9973 - accuracy: 0.7826 - val_loss: 0.7828 - val_accuracy: 0.8410\n",
      "Epoch 4/16\n",
      "8982/8982 [==============================] - 1s 96us/step - loss: 0.7796 - accuracy: 0.8329 - val_loss: 0.5899 - val_accuracy: 0.8810\n",
      "Epoch 5/16\n",
      "8982/8982 [==============================] - 1s 98us/step - loss: 0.6145 - accuracy: 0.8684 - val_loss: 0.4572 - val_accuracy: 0.9080\n",
      "Epoch 6/16\n",
      "8982/8982 [==============================] - 1s 98us/step - loss: 0.4864 - accuracy: 0.8958 - val_loss: 0.3556 - val_accuracy: 0.9300\n",
      "Epoch 7/16\n",
      "8982/8982 [==============================] - 1s 101us/step - loss: 0.3938 - accuracy: 0.9148 - val_loss: 0.2900 - val_accuracy: 0.9430\n",
      "Epoch 8/16\n",
      "8982/8982 [==============================] - 1s 98us/step - loss: 0.3227 - accuracy: 0.9287 - val_loss: 0.2301 - val_accuracy: 0.9570\n",
      "Epoch 9/16\n",
      "8982/8982 [==============================] - 1s 101us/step - loss: 0.2664 - accuracy: 0.9378 - val_loss: 0.1966 - val_accuracy: 0.9560\n",
      "Epoch 10/16\n",
      "8982/8982 [==============================] - 1s 101us/step - loss: 0.2304 - accuracy: 0.9424 - val_loss: 0.1687 - val_accuracy: 0.9630\n",
      "Epoch 11/16\n",
      "8982/8982 [==============================] - 1s 98us/step - loss: 0.2015 - accuracy: 0.9487 - val_loss: 0.1496 - val_accuracy: 0.9640\n",
      "Epoch 12/16\n",
      "8982/8982 [==============================] - 1s 99us/step - loss: 0.1791 - accuracy: 0.9508 - val_loss: 0.1346 - val_accuracy: 0.9680\n",
      "Epoch 13/16\n",
      "8982/8982 [==============================] - 1s 102us/step - loss: 0.1616 - accuracy: 0.9510 - val_loss: 0.1267 - val_accuracy: 0.9710\n",
      "Epoch 14/16\n",
      "8982/8982 [==============================] - 1s 99us/step - loss: 0.1525 - accuracy: 0.9540 - val_loss: 0.1131 - val_accuracy: 0.9680\n",
      "Epoch 15/16\n",
      "8982/8982 [==============================] - 1s 97us/step - loss: 0.1428 - accuracy: 0.9529 - val_loss: 0.1065 - val_accuracy: 0.9690\n",
      "Epoch 16/16\n",
      "8982/8982 [==============================] - 1s 97us/step - loss: 0.1370 - accuracy: 0.9534 - val_loss: 0.0969 - val_accuracy: 0.9680\n",
      "2246/2246 [==============================] - 0s 133us/step\n"
     ]
    }
   ],
   "source": [
    "# 모델 처음부터 다시 훈련\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 아까 처음에 훈련할때는 일부만 model.fit(partial_x_train, partial_y_train) 하고\n",
    "# 과대적합 어딘지 찾은 다음에는 전체인 (x_train, one_hot_train_labels)으로 하는 것이다!!\n",
    "\n",
    "model.fit(x_train, one_hot_train_labels,\n",
    "          epochs=16, batch_size=512, validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0760353256949122, 0.8000890612602234]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대략 80%의 정확도 달성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 109us/step - loss: 2.5451 - accuracy: 0.5564 - val_loss: 1.7853 - val_accuracy: 0.6580\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 99us/step - loss: 1.4495 - accuracy: 0.7200 - val_loss: 1.3378 - val_accuracy: 0.7220\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 104us/step - loss: 1.0645 - accuracy: 0.7876 - val_loss: 1.1384 - val_accuracy: 0.7630\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 97us/step - loss: 0.8341 - accuracy: 0.8335 - val_loss: 1.0197 - val_accuracy: 0.7890\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 97us/step - loss: 0.6710 - accuracy: 0.8672 - val_loss: 0.9448 - val_accuracy: 0.8040\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 100us/step - loss: 0.5485 - accuracy: 0.8914 - val_loss: 0.8929 - val_accuracy: 0.8140\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 99us/step - loss: 0.4515 - accuracy: 0.9088 - val_loss: 0.8650 - val_accuracy: 0.8190\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 98us/step - loss: 0.3769 - accuracy: 0.9235 - val_loss: 0.8389 - val_accuracy: 0.8220\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 97us/step - loss: 0.3161 - accuracy: 0.9340 - val_loss: 0.8277 - val_accuracy: 0.8290\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 99us/step - loss: 0.2717 - accuracy: 0.9404 - val_loss: 0.8249 - val_accuracy: 0.8240\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 98us/step - loss: 0.2339 - accuracy: 0.9456 - val_loss: 0.8329 - val_accuracy: 0.8210\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 99us/step - loss: 0.2053 - accuracy: 0.9488 - val_loss: 0.8395 - val_accuracy: 0.8290\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 98us/step - loss: 0.1815 - accuracy: 0.9516 - val_loss: 0.8395 - val_accuracy: 0.8230\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 98us/step - loss: 0.1634 - accuracy: 0.9525 - val_loss: 0.8737 - val_accuracy: 0.8170\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 98us/step - loss: 0.1502 - accuracy: 0.9541 - val_loss: 0.8603 - val_accuracy: 0.8240\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 98us/step - loss: 0.1378 - accuracy: 0.9551 - val_loss: 0.8744 - val_accuracy: 0.8190\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 99us/step - loss: 0.1296 - accuracy: 0.9568 - val_loss: 0.9114 - val_accuracy: 0.8210\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 96us/step - loss: 0.1218 - accuracy: 0.9568 - val_loss: 0.8947 - val_accuracy: 0.8220\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 99us/step - loss: 0.1152 - accuracy: 0.9573 - val_loss: 0.9137 - val_accuracy: 0.8190\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 96us/step - loss: 0.1109 - accuracy: 0.9570 - val_loss: 0.9355 - val_accuracy: 0.8170\n"
     ]
    }
   ],
   "source": [
    "# 추가 문제 (은닉층 하나로)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "# model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 컴파일을 한 후에 훈련을 검증해보는 것이다.\n",
    "# 훈련 데이터 (train)에서 1000개의 샘플을 떼어내서 검증(validation) 세트로 사용한다.\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "\n",
    "history = model.fit(partial_x_train,partial_y_train,\n",
    "                    epochs=20, batch_size=512, validation_data=(x_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 1000 samples\n",
      "Epoch 1/16\n",
      "8982/8982 [==============================] - 1s 104us/step - loss: 2.5311 - accuracy: 0.5354 - val_loss: 1.6455 - val_accuracy: 0.6810\n",
      "Epoch 2/16\n",
      "8982/8982 [==============================] - 1s 97us/step - loss: 1.4319 - accuracy: 0.7211 - val_loss: 1.0867 - val_accuracy: 0.7960\n",
      "Epoch 3/16\n",
      "8982/8982 [==============================] - 1s 97us/step - loss: 1.0293 - accuracy: 0.7923 - val_loss: 0.8029 - val_accuracy: 0.8480\n",
      "Epoch 4/16\n",
      "8982/8982 [==============================] - 1s 97us/step - loss: 0.7949 - accuracy: 0.8395 - val_loss: 0.6191 - val_accuracy: 0.8860\n",
      "Epoch 5/16\n",
      "8982/8982 [==============================] - 1s 98us/step - loss: 0.6355 - accuracy: 0.8749 - val_loss: 0.4907 - val_accuracy: 0.9130\n",
      "Epoch 6/16\n",
      "8982/8982 [==============================] - 1s 96us/step - loss: 0.5161 - accuracy: 0.8987 - val_loss: 0.3997 - val_accuracy: 0.9280\n",
      "Epoch 7/16\n",
      "8982/8982 [==============================] - 1s 98us/step - loss: 0.4251 - accuracy: 0.9144 - val_loss: 0.3318 - val_accuracy: 0.9420\n",
      "Epoch 8/16\n",
      "8982/8982 [==============================] - 1s 98us/step - loss: 0.3559 - accuracy: 0.9264 - val_loss: 0.2730 - val_accuracy: 0.9510\n",
      "Epoch 9/16\n",
      "8982/8982 [==============================] - 1s 95us/step - loss: 0.2999 - accuracy: 0.9353 - val_loss: 0.2365 - val_accuracy: 0.9520\n",
      "Epoch 10/16\n",
      "8982/8982 [==============================] - 1s 98us/step - loss: 0.2566 - accuracy: 0.9427 - val_loss: 0.2028 - val_accuracy: 0.9590\n",
      "Epoch 11/16\n",
      "8982/8982 [==============================] - 1s 97us/step - loss: 0.2234 - accuracy: 0.9471 - val_loss: 0.1789 - val_accuracy: 0.9620\n",
      "Epoch 12/16\n",
      "8982/8982 [==============================] - 1s 98us/step - loss: 0.1996 - accuracy: 0.9480 - val_loss: 0.1549 - val_accuracy: 0.9730\n",
      "Epoch 13/16\n",
      "8982/8982 [==============================] - ETA: 0s - loss: 0.1775 - accuracy: 0.95 - 1s 95us/step - loss: 0.1780 - accuracy: 0.9530 - val_loss: 0.1419 - val_accuracy: 0.9670\n",
      "Epoch 14/16\n",
      "8982/8982 [==============================] - 1s 94us/step - loss: 0.1627 - accuracy: 0.9522 - val_loss: 0.1277 - val_accuracy: 0.9700\n",
      "Epoch 15/16\n",
      "8982/8982 [==============================] - 1s 99us/step - loss: 0.1501 - accuracy: 0.9549 - val_loss: 0.1159 - val_accuracy: 0.9710\n",
      "Epoch 16/16\n",
      "8982/8982 [==============================] - 1s 98us/step - loss: 0.1386 - accuracy: 0.9548 - val_loss: 0.1170 - val_accuracy: 0.9670\n",
      "2246/2246 [==============================] - 0s 109us/step\n"
     ]
    }
   ],
   "source": [
    "# 모델 처음부터 다시 훈련\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "# model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 아까 처음에 훈련할때는 일부만 model.fit(partial_x_train, partial_y_train) 하고\n",
    "# 과대적합 어딘지 찾은 다음에는 전체인 (x_train, one_hot_train_labels)으로 하는 것이다!!\n",
    "\n",
    "model.fit(x_train, one_hot_train_labels,\n",
    "          epochs=16, batch_size=512, validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9396783148509948, 0.8076580762863159]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것도 80프로의 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 105us/step - loss: 3.0894 - accuracy: 0.4678 - val_loss: 2.3786 - val_accuracy: 0.5970\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 1.9733 - accuracy: 0.6495 - val_loss: 1.7110 - val_accuracy: 0.6560\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 86us/step - loss: 1.4479 - accuracy: 0.7091 - val_loss: 1.4022 - val_accuracy: 0.6990\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 1.1650 - accuracy: 0.7585 - val_loss: 1.2358 - val_accuracy: 0.7360\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 92us/step - loss: 0.9751 - accuracy: 0.7980 - val_loss: 1.1336 - val_accuracy: 0.7560\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.8316 - accuracy: 0.8252 - val_loss: 1.0630 - val_accuracy: 0.7700\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.7172 - accuracy: 0.8482 - val_loss: 1.0205 - val_accuracy: 0.7850\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 86us/step - loss: 0.6182 - accuracy: 0.8692 - val_loss: 0.9851 - val_accuracy: 0.8020\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.5338 - accuracy: 0.8879 - val_loss: 0.9719 - val_accuracy: 0.8000\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.4620 - accuracy: 0.9058 - val_loss: 0.9468 - val_accuracy: 0.8060\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 87us/step - loss: 0.4020 - accuracy: 0.9173 - val_loss: 0.9545 - val_accuracy: 0.8030\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.3494 - accuracy: 0.9287 - val_loss: 0.9337 - val_accuracy: 0.8170\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.3059 - accuracy: 0.9362 - val_loss: 0.9653 - val_accuracy: 0.8080\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.2713 - accuracy: 0.9424 - val_loss: 0.9476 - val_accuracy: 0.8120\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.2428 - accuracy: 0.9473 - val_loss: 0.9886 - val_accuracy: 0.8050\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.2181 - accuracy: 0.9476 - val_loss: 0.9795 - val_accuracy: 0.8040\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 91us/step - loss: 0.1950 - accuracy: 0.9501 - val_loss: 1.0132 - val_accuracy: 0.8030\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.1788 - accuracy: 0.9548 - val_loss: 0.9917 - val_accuracy: 0.8050\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 90us/step - loss: 0.1648 - accuracy: 0.9531 - val_loss: 0.9984 - val_accuracy: 0.8050\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 0.1511 - accuracy: 0.9545 - val_loss: 1.0775 - val_accuracy: 0.7970\n"
     ]
    }
   ],
   "source": [
    "# 추가 문제 (32개의 유닛)\n",
    "# 46차원보다 32가 더 작으니까 결과 더 안좋게 나오지 않을까..?\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 컴파일을 한 후에 훈련을 검증해보는 것이다.\n",
    "# 훈련 데이터 (train)에서 1000개의 샘플을 떼어내서 검증(validation) 세트로 사용한다.\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "\n",
    "history = model.fit(partial_x_train,partial_y_train,\n",
    "                    epochs=20, batch_size=512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18로 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 1000 samples\n",
      "Epoch 1/18\n",
      "8982/8982 [==============================] - 1s 107us/step - loss: 3.1757 - accuracy: 0.4463 - val_loss: 2.3822 - val_accuracy: 0.5990\n",
      "Epoch 2/18\n",
      "8982/8982 [==============================] - 1s 89us/step - loss: 2.0008 - accuracy: 0.6238 - val_loss: 1.5989 - val_accuracy: 0.6640\n",
      "Epoch 3/18\n",
      "8982/8982 [==============================] - 1s 90us/step - loss: 1.4572 - accuracy: 0.6902 - val_loss: 1.2299 - val_accuracy: 0.7310\n",
      "Epoch 4/18\n",
      "8982/8982 [==============================] - 1s 91us/step - loss: 1.1771 - accuracy: 0.7482 - val_loss: 1.0022 - val_accuracy: 0.7950\n",
      "Epoch 5/18\n",
      "8982/8982 [==============================] - 1s 92us/step - loss: 0.9885 - accuracy: 0.7914 - val_loss: 0.8460 - val_accuracy: 0.8250\n",
      "Epoch 6/18\n",
      "8982/8982 [==============================] - 1s 91us/step - loss: 0.8417 - accuracy: 0.8212 - val_loss: 0.7166 - val_accuracy: 0.8540\n",
      "Epoch 7/18\n",
      "8982/8982 [==============================] - 1s 92us/step - loss: 0.7208 - accuracy: 0.8466 - val_loss: 0.6038 - val_accuracy: 0.8730\n",
      "Epoch 8/18\n",
      "8982/8982 [==============================] - 1s 89us/step - loss: 0.6176 - accuracy: 0.8705 - val_loss: 0.5165 - val_accuracy: 0.8980\n",
      "Epoch 9/18\n",
      "8982/8982 [==============================] - 1s 89us/step - loss: 0.5311 - accuracy: 0.8887 - val_loss: 0.4373 - val_accuracy: 0.9120\n",
      "Epoch 10/18\n",
      "8982/8982 [==============================] - 1s 88us/step - loss: 0.4585 - accuracy: 0.9019 - val_loss: 0.3742 - val_accuracy: 0.9290\n",
      "Epoch 11/18\n",
      "8982/8982 [==============================] - 1s 88us/step - loss: 0.3976 - accuracy: 0.9157 - val_loss: 0.3240 - val_accuracy: 0.9380\n",
      "Epoch 12/18\n",
      "8982/8982 [==============================] - 1s 88us/step - loss: 0.3455 - accuracy: 0.9243 - val_loss: 0.2876 - val_accuracy: 0.9350\n",
      "Epoch 13/18\n",
      "8982/8982 [==============================] - 1s 90us/step - loss: 0.3048 - accuracy: 0.9299 - val_loss: 0.2507 - val_accuracy: 0.9480\n",
      "Epoch 14/18\n",
      "8982/8982 [==============================] - 1s 90us/step - loss: 0.2692 - accuracy: 0.9382 - val_loss: 0.2231 - val_accuracy: 0.9570\n",
      "Epoch 15/18\n",
      "8982/8982 [==============================] - 1s 91us/step - loss: 0.2411 - accuracy: 0.9431 - val_loss: 0.1962 - val_accuracy: 0.9630\n",
      "Epoch 16/18\n",
      "8982/8982 [==============================] - 1s 89us/step - loss: 0.2186 - accuracy: 0.9464 - val_loss: 0.1739 - val_accuracy: 0.9640\n",
      "Epoch 17/18\n",
      "8982/8982 [==============================] - 1s 92us/step - loss: 0.1996 - accuracy: 0.9483 - val_loss: 0.1579 - val_accuracy: 0.9670\n",
      "Epoch 18/18\n",
      "8982/8982 [==============================] - 1s 90us/step - loss: 0.1820 - accuracy: 0.9498 - val_loss: 0.1543 - val_accuracy: 0.9670\n",
      "2246/2246 [==============================] - 0s 95us/step\n"
     ]
    }
   ],
   "source": [
    "# 모델 처음부터 다시 훈련\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 아까 처음에 훈련할때는 일부만 model.fit(partial_x_train, partial_y_train) 하고\n",
    "# 과대적합 어딘지 찾은 다음에는 전체인 (x_train, one_hot_train_labels)으로 하는 것이다!!\n",
    "\n",
    "model.fit(x_train, one_hot_train_labels,\n",
    "          epochs=18, batch_size=512, validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0534891143826004, 0.7898486256599426]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "78%의 정확도. 예상대로 유닛 사이즈가 출력해야할 클래스(레이블)의 개수보다 작아지니까 정확도가 감소했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 137us/step - loss: 2.1996 - accuracy: 0.5430 - val_loss: 1.3504 - val_accuracy: 0.6990\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 118us/step - loss: 1.0970 - accuracy: 0.7650 - val_loss: 1.0641 - val_accuracy: 0.7850\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 126us/step - loss: 0.7763 - accuracy: 0.8359 - val_loss: 0.9474 - val_accuracy: 0.8130\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 120us/step - loss: 0.5491 - accuracy: 0.8864 - val_loss: 0.9122 - val_accuracy: 0.8040\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 121us/step - loss: 0.4038 - accuracy: 0.9187 - val_loss: 0.8764 - val_accuracy: 0.8130\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 121us/step - loss: 0.3158 - accuracy: 0.9312 - val_loss: 0.8498 - val_accuracy: 0.8240\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 123us/step - loss: 0.2422 - accuracy: 0.9446 - val_loss: 0.8849 - val_accuracy: 0.8200\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 119us/step - loss: 0.2057 - accuracy: 0.9484 - val_loss: 0.9250 - val_accuracy: 0.8250\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 121us/step - loss: 0.1813 - accuracy: 0.9513 - val_loss: 0.9676 - val_accuracy: 0.8100\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 124us/step - loss: 0.1564 - accuracy: 0.9551 - val_loss: 0.9658 - val_accuracy: 0.8130\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 119us/step - loss: 0.1493 - accuracy: 0.9540 - val_loss: 1.0130 - val_accuracy: 0.8080\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 120us/step - loss: 0.1381 - accuracy: 0.9554 - val_loss: 0.9690 - val_accuracy: 0.8150\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 127us/step - loss: 0.1308 - accuracy: 0.9570 - val_loss: 1.0079 - val_accuracy: 0.8090\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 122us/step - loss: 0.1268 - accuracy: 0.9563 - val_loss: 1.0200 - val_accuracy: 0.8080\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 121us/step - loss: 0.1228 - accuracy: 0.9563 - val_loss: 1.0724 - val_accuracy: 0.8030\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 122us/step - loss: 0.1174 - accuracy: 0.9557 - val_loss: 1.0239 - val_accuracy: 0.8110\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 121us/step - loss: 0.1169 - accuracy: 0.9555 - val_loss: 1.0908 - val_accuracy: 0.8090\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 123us/step - loss: 0.1093 - accuracy: 0.9567 - val_loss: 1.1227 - val_accuracy: 0.7920\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 122us/step - loss: 0.1064 - accuracy: 0.9588 - val_loss: 1.1430 - val_accuracy: 0.7960\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 120us/step - loss: 0.1023 - accuracy: 0.9565 - val_loss: 1.0853 - val_accuracy: 0.7940\n"
     ]
    }
   ],
   "source": [
    "# 추가 문제 (128개의 유닛)\n",
    "# 46차원과 너무 차이가 나니까 결과 더 안좋게 나오지 않을까..?\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 컴파일을 한 후에 훈련을 검증해보는 것이다.\n",
    "# 훈련 데이터 (train)에서 1000개의 샘플을 떼어내서 검증(validation) 세트로 사용한다.\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "\n",
    "history = model.fit(partial_x_train,partial_y_train,\n",
    "                    epochs=20, batch_size=512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10으로 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "8982/8982 [==============================] - 1s 136us/step - loss: 2.0768 - accuracy: 0.5794 - val_loss: 1.1489 - val_accuracy: 0.7480\n",
      "Epoch 2/10\n",
      "8982/8982 [==============================] - 1s 123us/step - loss: 1.0384 - accuracy: 0.7704 - val_loss: 0.7378 - val_accuracy: 0.8540\n",
      "Epoch 3/10\n",
      "8982/8982 [==============================] - 1s 121us/step - loss: 0.7200 - accuracy: 0.8440 - val_loss: 0.4919 - val_accuracy: 0.9030\n",
      "Epoch 4/10\n",
      "8982/8982 [==============================] - 1s 121us/step - loss: 0.5159 - accuracy: 0.8940 - val_loss: 0.3740 - val_accuracy: 0.9250\n",
      "Epoch 5/10\n",
      "8982/8982 [==============================] - 1s 124us/step - loss: 0.3891 - accuracy: 0.9167 - val_loss: 0.2720 - val_accuracy: 0.9560\n",
      "Epoch 6/10\n",
      "8982/8982 [==============================] - 1s 121us/step - loss: 0.2889 - accuracy: 0.9365 - val_loss: 0.2549 - val_accuracy: 0.9500\n",
      "Epoch 7/10\n",
      "8982/8982 [==============================] - 1s 119us/step - loss: 0.2361 - accuracy: 0.9437 - val_loss: 0.1690 - val_accuracy: 0.9660\n",
      "Epoch 8/10\n",
      "8982/8982 [==============================] - 1s 124us/step - loss: 0.2081 - accuracy: 0.9451 - val_loss: 0.1388 - val_accuracy: 0.9730\n",
      "Epoch 9/10\n",
      "8982/8982 [==============================] - 1s 122us/step - loss: 0.1757 - accuracy: 0.9503 - val_loss: 0.1325 - val_accuracy: 0.9690\n",
      "Epoch 10/10\n",
      "8982/8982 [==============================] - 1s 121us/step - loss: 0.1675 - accuracy: 0.9522 - val_loss: 0.1401 - val_accuracy: 0.9660\n",
      "2246/2246 [==============================] - 0s 126us/step\n"
     ]
    }
   ],
   "source": [
    "# 모델 처음부터 다시 훈련\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 아까 처음에 훈련할때는 일부만 model.fit(partial_x_train, partial_y_train) 하고\n",
    "# 과대적합 어딘지 찾은 다음에는 전체인 (x_train, one_hot_train_labels)으로 하는 것이다!!\n",
    "\n",
    "model.fit(x_train, one_hot_train_labels,\n",
    "          epochs=10, batch_size=512, validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0253722852399492, 0.7978628873825073]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도 79%로 감소했다., 예상대로 출력 클래스와 층의 차원이 너무 차이가 나면 정확도가 감소하는 것 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
